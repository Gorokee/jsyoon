
<html class="gr_umn_edu"><head>
<meta name="google-site-verification" content="8BqG5WT5qAorBC_EV7_zYzk5DWc_n2WDgcRnTgQJln4" />
<meta http-equiv="Content-Type" content="text/html; charset=utf-8">
<link rel="shortcut icon" href="./JaeShin_homepage/goldyfacingR-lg.gif" type="image/x-icon">
<title> Jae Shin Yoon</title>
<meta name="description" content=" I am a Ph.D. candidate in Computer Science and Engineering at the University of Minnesota (UMN), Twin Cities, advised by Prof. Hyun Soo Park.  I received MS degree in Department of Robotics from Korea Advanced Institute of Science and Technology (KAIST) (2015-2017) under the supervision of Prof. In So Kweon. I received my bachelor degree in Electrical Engineering from Hanyang University (2009-2014) under the supervision of Prof. Yong Ho Song. During 2013, I was an exchange student at Kobe University. My general research interest lies in computer&robot vision and machine learning.">
<meta name="keywords" content="jae shin yoon, computer science, computer vision, machine learning, university of minnesota, hyun soo park">

<link href="./JaeShin_homepage/css" rel="stylesheet" type="text/css">
<link href="./JaeShin_homepage/css(1)" rel="stylesheet" type="text/css">
<!--
@import url(http://fonts.googleapis.com/earlyaccess/nanumgothicoding.css);
-->
<link media="all" href="./JaeShin_homepage/default2.css" type="text/css" rel="StyleSheet">




<script language="javascript" type="text/javascript">

function toggleHide(id) 
{
  if (document.getElementById)
  {
    obj = document.getElementById(id);
    if (obj.style.display == "none")
    {
      obj.style.display = "";
    } 
    else 
    {
      obj.style.display = "none";
    }
  }
}  
</script> 



</head>




<body data-gr-c-s-loaded="true">

<div id="container">

<table border="0">
<tbody><tr>

<script language="JavaScript">
showImage();
</script><img src="./JaeShin_homepage/jae_shin.jpg" class="right" border="0" width="230" height="345" vspace="0">

<td width="80%" height="10px">
<h1>
</h1><h1><a href="https://gorokee.github.io/jsyoon" style="color: black; font-weight: bold; padding-left: 10px">Jae Shin Yoon (윤재신) </a></h1>

</td>

</tr>
</tbody></table>

&nbsp;&nbsp;&nbsp;



<p style="font-weight:400">


<font size="3.5">
I am a Research Scientist at <a href="https://research.adobe.com/">Adobe Research</a>.
<br><br>
<br>
I received Ph.D. degree in <a href="https://www.cs.umn.edu/">Computer Science and Engineering</a> at the <a href="https://twin-cities.umn.edu/">University of Minnesota (UMN), Twin Cities</a>, advised by <a href="http://www-users.cs.umn.edu/~hspark/">Prof. Hyun Soo Park</a>.  I received MS degree in <a href="http://kchannel.kaist.ac.kr/CH500-012">Department of Robotics</a> from <a href="http://www.kaist.ac.kr/html/kr/index.html">Korea Advanced Institute of Science and Technology (KAIST)</a> under the supervision of <a href="http://rcv.kaist.ac.kr/">Prof. In So Kweon</a>. I received my bachelor degree in <a href="http://electronic.hanyang.ac.kr/en/index.php">Electrical Engineering</a> from <a href="http://www.hanyang.ac.kr/">Hanyang University</a> under the supervision of <a href="http://site.enc.hanyang.ac.kr/members/faculty">Prof. Yong Ho Song</a>. I was an exchange student at <a href="http://www.kobe-u.ac.jp/en/">Kobe University</a>. My general research interest lies in computer&robot vision and machine learning.


<br><br><br>
Email: jaeyoon [at] adobe.com, jsyoon4325 [at] gmail.com<br> 
<a href="https://github.com/Gorokee/jsyoon/blob/master/Jae%20Shin%20Yoon%20(CV).pdf" target="_blank">CV [07/10/2022]</a> | 
<a href="https://scholar.google.com/citations?user=jsC42nUAAAAJ&hl=en" target="_blank">Google Scholar </a>  |
<a href="https://www.humbi-data.net/" target="_blank">Human Behavioral Imaging Dataset</a>
<br><br><br>
<b>University Collaboration</b>: I am open to research collaboration. Please contact me if you are interested in.
<br>
<!-- 
<br><br><br>
<b>University Collaboration</b>: I am open to research collaboration. Please contact me if you are interested in.
<br>
<b>Internship</b>: Please contact me with your resume if you are interested in Research Intern at Adobe Research.
<br><br>
-->
<!-- <a href="https://sites.google.com/a/umn.edu/jaeshin/home" target="_blank">Course Website (Graphics)-->   

</p>



<h2>
<font size = "4">Research Highlight
</h2>

<br>
<table width="1000" border="0">
   <tr>

	   
    <td width="310", align="center"><font size = "2" font="font" face = "Arial, Helvetica">
   Project MotionMix <b>(Max Sneak 2022)</b> 
   <iframe width="310" height="190" src="https://www.youtube.com/embed/nK6olXpOh1g" title="#ProjectMotionMix | Adobe MAX Sneaks 2022" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
  </td>	   
	   
    <td width="310", align="center"><font size = "2" font="font" face = "Arial, Helvetica">
   <a href="https://arxiv.org/pdf/2203.12780.pdf" target="_new"> Motion Dependent Appearance from a Single Camera</a> <b>(CVPR 2022)</b> 
 <a href="https://gorokee.github.io/jsyoon/Motion_learning/" target="_new">[<b>Project page</b>]</a>
 <iframe width="310" height="190" src="https://www.youtube.com/embed/NWg1mRl22Ow" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
  </td>


    <td width="310", align="center"><font size = "2" font="font" face = "Arial, Helvetica">
   <a href="https://arxiv.org/pdf/2012.03796.pdf" target="_new"> Human Animation from a Single Image</a> <b>(CVPR 2021)</b> <div> 
 <a href="http://gvv.mpi-inf.mpg.de/projects/PoseGuidedHumanAnimation/" target="_new">[<b>Project page</b>]</a>
	<iframe width="310" height="190" src="https://www.youtube.com/embed/x7H0kKWzRFU" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
  </td>




	</tr>
 </table>


   <br>
<table width="1000" border="0">
   <tr>
   <tr>

    <td width="310", align="center"><font size = "2" font="font" face = "Arial, Helvetica">
   <a href="https://gorokee.github.io/jsyoon//JaeShin_homepage/dynamic_view.pdf" target="_new"> Dynamic Scenes Novel View Synthesis</a> <b>(CVPR 2020)</b> <div> 
 <a href="https://gorokee.github.io/jsyoon//dynamic_synth/" target="_new">[<b>Project page</b>]</a>
	<iframe width="310" height="190" src="https://www.youtube.com/embed/pTCkCGr2IH0" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
</td>	   
	   
	   
   <td width="310", align="center">
	<font size = "2" font="font" face = "Arial, Helvetica">
  <a href="https://arxiv.org/abs/1812.00281" target="_new"> HUMBI: A Large Multiview Human Behavioral Imaging Dataset </a><b>(CVPR 2020)</b>
 <a href="https://humbiorg.wordpress.com/" target="_new">[<b>Project page</b>]</a>
<iframe width="310" height="190" src="https://www.youtube.com/embed/1iVjFgYq8ZU" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>    </td>



<td width="310" , align="center"><font size = "2" font="font" face = "Arial, Helvetica">
   <a href="http://openaccess.thecvf.com/content_CVPR_2019/html/Yoon_Self-Supervised_Adaptation_of_High-Fidelity_Face_Models_for_Monocular_Performance_Tracking_CVPR_2019_paper.html" target="_new">Monocular 3D Face Performance Tracking</a> <b>(CVPR 2019)</b>  &nbsp; <a href="https://gorokee.github.io/jsyoon/3dface/">[<b>Project page</b>]</a><div> 
  <iframe width="310" height="190" src="https://www.youtube.com/embed/4KgO54JvOE0" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
   </td>

 
	</tr>
 </table>



 <br>
 <table width="1000" border="0">

	 
    <td width="310", align="center"><font size = "2" font="font" face = "Arial, Helvetica">
     <a href="./JaeShin_homepage/SemanticTrajectory.pdf" target="_new">3D Semantic Trajectory Reconstruction in the Pixel Continuum</a> <b>(CVPR 2018)</b>  &nbsp; <a href="https://gorokee.github.io/jsyoon/Semantic_trajectory/">[<b>Project page</b>]</a><div>
    <iframe width="310" height="190" src="https://www.youtube.com/embed/Mdi6Ljsi0DA" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>

</td>
	 
	 
	 
    <td width="310", align="center"><font size = "2" font="font" face = "Arial, Helvetica">
   <a href="https://arxiv.org/abs/1708.05137">Pixel-level Matching for Video Object Segmentation</a> <b>(ICCV 2017)</b> &nbsp; <a href="http://jsyoon4325.wixsite.com/pix-matching">[<b>Project page</b>]</a><div>
<iframe width="310" height="190" src="https://www.youtube.com/embed/oAM44Cl9BxI" frameborder="0" gesture="media" allow="encrypted-media" allowfullscreen></iframe>
	</td>


	    
	    
    <td width="310", align="center"><font size = "2" font="font" face = "Arial, Helvetica">
     <a href="https://arxiv.org/abs/1710.06288">Vanishing Point Guided Visual Perception for Autonomous Driving</a> <b>(ICCV 2017)</b> &nbsp;<a href="https://github.com/SeokjuLee/VPGNet">[<b>Project page</b>]</a><div>
   <iframe width="310" height="190" src="https://www.youtube.com/embed/jnewRlt6UbI" frameborder="0" gesture="media" allow="encrypted-media" allowfullscreen></iframe>	

</td>


 
	    
</tr>
   </table>



	




<a name="pub"></a>
<h2>
<font size = "4">Publications
</h2>


<!--
<font color='blue'><b>[Oral]</b></font>-->

<p style="margin-bottom: -20px;"></p>
<table style="width: 104%" border="0" cellspacing="0">



<tr>
<td width="10%">
<img src="./JaeShin_homepage/junuk_wacv2.gif" width="200" height="121">
</td>
<td width="90%" bgcolor="#e9e9e9"><div id="pub_gray"> 
<h5>3D Reconstruction of Interacting Multi-Person in Clothing from a Single Image</h5>
<a href="https://sites.google.com/view/junuk-cha/">Junuk Cha</a>, Hansol Lee, Jaewon Kim, Bao Truong, { <b>Jae Shin Yoon*</b>, <a href="https://sites.google.com/site/bsrvision00/">Seungryul Baek*</a> }<br>
*indicates joint last authors<br>	
IEEE Winter Conf. on Applications of Computer Vision (<b>WACV</b>) 2024 <br>
[PDF]
[Video]	
[Project Page]
</div></td>
</tr>


	
<tr>
<td width="10%">
<img src="./JaeShin_homepage/banner.png"  width="200" height="121">
</td>
<td width="90%" bgcolor="#ffffff"><div id="pub"> 
<h5>Complete 3D Human Reconstruction from a Single Incomplete Image</h5>
<a href="https://junyingw.github.io//">Junying Wang</a>, <b>Jae Shin Yoon</b>, <a href="https://tuanfeng.github.io/">Tuanfeng Y. Wang</a>, <a href="https://krsingh.cs.ucdavis.edu//">Krishna Kumar Singh</a>, and <a href="https://cgit.usc.edu/contact/ulrich-neumann//">Ulrich Neumann</a><br>
IEEE Computer Vision and Pattern Recognition (<b>CVPR</b>) 2023 <br>
<a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Wang_Complete_3D_Human_Reconstruction_From_a_Single_Incomplete_Image_CVPR_2023_paper.pdf">[PDF]</a> 
<a href="https://www.youtube.com/watch?v=6lMbnZp6vNQ&t=478s">[Video]</a> 
<a href="https://junyingw.github.io/paper/3d_inpainting/index.html">[Project page]</a>	
</div></td>
</tr>

	
	
	
	
	
	
	
<tr>
<td width="10%">
<img src="./JaeShin_homepage/Teaser.jpg" width="200" height="112">
</td>
<td width="90%" bgcolor="#e9e9e9"><div id="pub_gray"> 
<h5>Metaverse in the Wild: Modeling, Adapting, and Rendering of 3D Human Avatars from a Single Camera</h5>
<b>Jae Shin Yoon</b> <br>
PhD Thesis, Computer Science and Engineering, University of Minnesota 2022<br>
<a href="https://www.dropbox.com/s/9qq7v9ykku1p2wr/JaeShinYoon_PhD_Thesis.pdf?dl=0">[PDF]</a> 
</div></td>
</tr>
	
	
<tr>
<td width="10%">
<img src="./JaeShin_homepage/cvpr2022.gif"  width="200" height="121">
</td>
<td width="90%" bgcolor="#ffffff"><div id="pub"> 
<h5>Learning Motion-Dependent Appearance for High-Fidelity Rendering of Dynamic Humans from a Single Camera</h5>
<b>Jae Shin Yoon</b>, <a href="http://www.duygu-ceylan.com/">Duygu Ceylan</a>, <a href="https://tuanfeng.github.io/">Tuanfeng Y. Wang</a>, <a href="https://research.adobe.com/person/jingwan-lu/">Jingwan (Cynthia) Lu</a>, <a href="https://eng.ucmerced.edu/people/jyang44/">Jimei Yang</a>, <a href="https://zhixinshu.github.io/">Zhixin Shu</a>, and <a href="http://www-users.cs.umn.edu/~hspark/">Hyun Soo Park</a> <br>
IEEE Computer Vision and Pattern Recognition (<b>CVPR</b>) 2022 <br>
<a href="https://arxiv.org/pdf/2203.12780.pdf" target="_new">[PDF]</a> 
<a href="https://www.youtube.com/watch?v=NWg1mRl22Ow" target="_new">[Video] 
<a href="https://gorokee.github.io/jsyoon/Motion_learning/" target="_new">[Project page]
<a href="https://www.theverge.com/2022/10/19/23412526/adobe-dancing-ai-prototype-project-motion-mix-max-2022/" target="_new">[Article]	
<a href="javascript:toggleHide(&#39;motion3d&#39;)">[BibTeX] </a>
<pre id="motion3d" class="bibtex" style="display:none">@inproceedings{yoon2022motion,
  title={Learning Motion-Dependent Appearance for High-Fidelity Rendering of Dynamic Humans from a Single Camera},
  author={Yoon, Jae Shin and Ceylan, Duygu and  Wang, Tuanfeng Y. and  Lu, Jingwan and  Yang, Jimei  and Shu, Zhixin and Park, Hyun Soo},
  booktitle={CVPR},
  year={2022}
}
</pre>
</div></td>
</tr>





<tr>

<td width="10%">
<img src="./JaeShin_homepage/humbi.gif" width="200" height="112">
</td>
<td width="90%" bgcolor="#e9e9e9"><div id="pub_gray"> 
<h5>HUMBI: A Large Multiview Dataset of Human Body Expressions and Benchmark Challenge</h5>
<b>Jae Shin Yoon</b>, <a href="https://sites.google.com/umn.edu/zhixuany/">Zhixuan Yu</a>, <a href="https://jaesik.info/">Jaesik Park</a>, and <a href="http://www-users.cs.umn.edu/~hspark/">Hyun Soo Park</a><br>
IEEE Transactions on Pattern Analysis and Machine Intelligence (<b>TPAMI</b>) <font color='red'><b>Journal</b></font><br>
<a href="https://arxiv.org/pdf/2110.00119.pdf">[PDF]</a> (Extended version of CVPR 20)
<a href="https://humbi-data.net/">[Dataset Page]</a>
<a href="javascript:toggleHide(&#39;humbi_pami&#39;)">[BibTeX] </a>
<pre id="humbi_pami" class="bibtex" style="display:none">@article{yoon2021humbi,
  title={HUMBI: A Large Multiview Dataset of Human Body Expressions and Benchmark Challenge},
  author={Yoon, Jae Shin and Yu, Zhixuan and Park, Jaesik and Park, Hyun Soo},
  journal={TPAMI},
  year={2021}
}

</pre>
</div></td>
</tr>



<!-- 2020 -->

<tbody><tr><td>
<!--
<h6>2021</h6>
-->
</td>

<tr>

<td width="10%">
<img src="./JaeShin_homepage/human_ani.gif"  width="200" height="105">
</td>
<td width="90%" bgcolor="#ffffff"><div id="pub"> 
<h5>Pose-Guided Human Animation from a Single Image in the Wild</h5>
<b>Jae Shin Yoon</b>, <a href="https://lingjie0206.github.io/">Lingjie Liu</a>, <a href="https://people.mpi-inf.mpg.de/~golyanik/">Vladislav Golyanik</a>, <a href="https://people.mpi-inf.mpg.de/~ksarkar/">Kripasindhu Sarkar</a>, <a href="https://www-users.cs.umn.edu/~hspark/">Hyun Soo Park</a>, and <a href="http://gvv.mpi-inf.mpg.de/GVV_Team.html">Christian Theobalt</a><br>
IEEE Computer Vision and Pattern Recognition (<b>CVPR</b>) 2021 <br>
<a href="https://arxiv.org/pdf/2012.03796.pdf" target="_new">[PDF]</a> <a href="https://www.youtube.com/watch?v=x7H0kKWzRFU&t=4s" target="_new">[Video] 
<a href="http://gvv.mpi-inf.mpg.de/projects/PoseGuidedHumanAnimation/" target="_new">[Project page]</a>
<a href="https://github.com/Gorokee/humanani.git" target="_new">[Code]</a>
<a href="https://www.dropbox.com/s/yyj8y7tw0hljj7o/poster.pdf?dl=0" target="_new">[Poster]</a>
<a href="https://www.dropbox.com/s/6fej7m0zt3zdw7c/video.mp4?dl=0" target="_new">[5-min video]</a>
</div></td>
</tr>







<tbody><tr><td>
<!--
<h6>2020</h6>
-->

</td>


<tr>

<td width="10%">
<img src="./JaeShin_homepage/view_syn-min.gif" width="200" height="112">
</td>
<td width="90%" bgcolor="#e9e9e9"><div id="pub_gray"> 
<h5>Novel View Synthesis of Dynamic Scenes with Globally Coherent Depths from a Monocular Camera</h5>
<b>Jae Shin Yoon</b>, <a href="http://www.kihwan23.com/">Kihwan Kim</a>, <a href="http://alumni.soe.ucsc.edu/~orazio/">Orazio Gallo</a>, <a href="http://www-users.cs.umn.edu/~hspark/">Hyun Soo Park</a>, and <a href="http://jankautz.com/">Jan Kautz</a><br>
IEEE Computer Vision and Pattern Recognition (<b>CVPR</b>) 2020 <br>
<a href="./JaeShin_homepage/dynamic_view.pdf">[PDF]</a>
<a href="./JaeShin_homepage/dynamic_view_supple.pdf">[PDF_supple]</a>
<a href="https://youtu.be/brza-yB7cVE" target="_blank">[Video]</a>
<a href="https://youtu.be/pTCkCGr2IH0" target="_blank">[Video_supple]</a>
<a href="https://gorokee.github.io/jsyoon/dynamic_synth/">[Project Page]
</div></td>
</tr>



<tr>
<td width="10%">
<img src="./JaeShin_homepage/humbi.gif"  width="200" height="100">
</td>
<td width="90%" bgcolor="#ffffff"><div id="pub"> 
<h5>HUMBI: A Large Multiview Dataset of Human Body Expressions</h5>
{<a href="https://sites.google.com/umn.edu/zhixuany">Zhixuan Yu</a>*, <b>Jae Shin Yoon*</b>}, In Kyu Lee, Prashanth Venkatesh, <a href="http://jaesik.info/">Jaesik Park</a>, Jihun Yu, and <a href="http://www-users.cs.umn.edu/~hspark/">Hyun Soo Park</a><br>
*indicates joint first authors<br>
IEEE Computer Vision and Pattern Recognition (<b>CVPR</b>) 2020 <br>
<a href="https://openaccess.thecvf.com/content_CVPR_2020/papers/Yu_HUMBI_A_Large_Multiview_Dataset_of_Human_Body_Expressions_CVPR_2020_paper.pdf" target="_new">[PDF]</a> <a href="https://youtu.be/1iVjFgYq8ZU" target="_new">[Video] <a href="https://www.humbi-data.net/" target="_new">[Project Page]
</div></td>
</tr>



<!-- 2019 -->

<tbody><tr><td>
<!-- 
<h6>2019</h6>
-->

</td>

<tr>

<td width="10%">
<img src="./JaeShin_homepage/3dtrack4.gif" width="200" height="112">
</td>
<td width="90%" bgcolor="#e9e9e9"><div id="pub_gray"> 
<h5>Self-Supervised Adaptation of High-Fidelity Face Models for Monocular Performance Tracking</h5>
<b>Jae Shin Yoon</b>, <a href="https://scholar.google.com/citations?user=YvS3QpkAAAAJ&hl=en">Takaaki Shiratori</a>, <a href="https://sites.google.com/view/shoou-i-yu/home">Shoou-I Yu</a>, and <a href="http://www-users.cs.umn.edu/~hspark/">Hyun Soo Park</a><br>
IEEE Computer Vision and Pattern Recognition (<b>CVPR</b>) 2019 <font color='red'><b>Oral presentation</b></font><br>
<a href="http://openaccess.thecvf.com/content_CVPR_2019/html/Yoon_Self-Supervised_Adaptation_of_High-Fidelity_Face_Models_for_Monocular_Performance_Tracking_CVPR_2019_paper.html" target="_new">[PDF] </a> <a href="http://openaccess.thecvf.com/content_CVPR_2019/html/Yoon_Self-Supervised_Adaptation_of_High-Fidelity_Face_Models_for_Monocular_Performance_Tracking_CVPR_2019_paper.html" target="_new">[Supple]</a>
<a href="https://youtu.be/4KgO54JvOE0" target="_blank">[Video]</a>
<a href="https://gorokee.github.io/jsyoon/3dface/" target="_new">[Project Page]</a>
<a href="./JaeShin_homepage/poster_3dface.pdf" target="_new">[Poster] </a> 
</div></td>
</tr>

<!--
<td width="10%">
<img src="./JaeShin_homepage/3dtrack4.gif" width="200" height="112">
</td>
<td width="90%" bgcolor="#ffffff"><div id="pub"> 
<h5>Self-Supervised Adaptation of High-Fidelity Face Models for Monocular Performance Tracking</h5>
<b>Jae Shin Yoon</b>, <a href="https://scholar.google.com/citations?user=YvS3QpkAAAAJ&hl=en">Takaaki Shiratori</a>, <a href="https://sites.google.com/view/shoou-i-yu/home">Shoou-I Yu</a>, and <a href="http://www-users.cs.umn.edu/~hspark/">Hyun Soo Park</a><br>
IEEE Computer Vision and Pattern Recognition (<b>CVPR</b>) 2019 <font color='red'><b>[Oral presentation]</b></font><br>
<a href="http://openaccess.thecvf.com/content_CVPR_2019/html/Yoon_Self-Supervised_Adaptation_of_High-Fidelity_Face_Models_for_Monocular_Performance_Tracking_CVPR_2019_paper.html" target="_new">[PDF] </a> <a href="http://openaccess.thecvf.com/content_CVPR_2019/html/Yoon_Self-Supervised_Adaptation_of_High-Fidelity_Face_Models_for_Monocular_Performance_Tracking_CVPR_2019_paper.html" target="_new">[Supple]</a>
<a href="https://youtu.be/4KgO54JvOE0" target="_blank">[Video]</a>
<a href="https://gorokee.github.io/jsyoon/3dface/" target="_new">[Project Page]</a>
<a href="./JaeShin_homepage/poster_3dface.pdf" target="_new">[Poster] </a> 
<a href="javascript:toggleHide(&#39;bibArxiv17_ssmn&#39;)">[BibTeX] </a>
<pre id="bibArxiv17_ssmn" class="bibtex" style="display:none">@InProceedings{Yoon_2019_CVPR,
author = {Yoon, Jae Shin and Shiratori, Takaaki and Yu, Shoou-I and Park, Hyun Soo},
title = {Self-Supervised Adaptation of High-Fidelity Face Models for Monocular Performance Tracking},
booktitle = {The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
month = {June},
year = {2019}
}
</pre>
</div></td>
</tr>
-->


<!--


-->




<!-- 2018 -->

<tbody><tr><td>
<!--
<h6>2018</h6>
-->
</td>

<tr>



<td width="10%">
<img src="./JaeShin_homepage/seman_traject.gif" width="200" height="100">
</td>
<td width="90%" bgcolor="#ffffff"><div id="pub"> 
<h5>3D Semantic Trajectory Reconstruction from 3D Pixel Continuum</h5>
<b>Jae Shin Yoon</b>, Ziwei Li, <a href="http://www-users.cs.umn.edu/~hspark/">Hyun Soo Park</a><br>
IEEE Computer Vision and Pattern Recognition (<b>CVPR</b>) 2018 <br>
<a href="./JaeShin_homepage/SemanticTrajectory.pdf" target="_new">[PDF] </a> 
<a href="https://youtu.be/Mdi6Ljsi0DA" target="_new">[Video] </a> 
<a href="https://gorokee.github.io/jsyoon/Semantic_trajectory/" target="_new">[Project Page]</a>
</div></td>
</tr>

<tr>
<td width="10%">
<img src="./JaeShin_homepage/tits2.PNG"  width="200" height="100">
</td>
<td width = "90%" bgcolor="#e9e9e9"> <div id="pub_gray">
<h5>KAIST Multi-spectral Day/Night Dataset for Autonomous and Assisted Driving</h5>
Yukyung Choi, Namil Kim, Soonmin Hwang, Kibaek Park, <b>Jae Shin Yoon</b>, Kyounghwan An, and In So Kweon<br>
Transactions on Intelligent Transportation Systems (<b>T-ITS</b>)<br>
<a href="./JaeShin_homepage/kaist_multispectral.pdf" target="_new">[PDF] <a href="https://sites.google.com/view/multispectral/home" target="-new">[Project Page]</a>
</div></td>
</tr>


<tr><td>
<!--
<h6>2017</h6>
-->
</td></tr>

<!-- <font color="blue"><b>[Spotlight]</b></font> 
 -->

<tr>
<td width="10%">
<img src="JaeShin_homepage/pix_matching.png" width="200" height="100">
</td>
<td width="90%" bgcolor="#ffffff"><div id="pub"> 
<h5>Pixel-level Matching for Video Object Segmentation using Convolutional Neural Networks</h5>
<b>Jae Shin Yoon</b>, Francois Rameau, Junsik Kim, Seokju Lee, Seunghak Shin, and In So Kweon<br>
IEEE International Conference on Computer Vision (<b>ICCV</b>) 2017 <br>
<a href="https://arxiv.org/abs/1708.05137" target="_new">[PDF] </a>
<a href="https://jsyoon4325.wixsite.com/pix-matching" target="_new">[Project Page] </a>
<a href="http://docs.wixstatic.com/ugd/6db7b6_3e954d46bcbf4aeea6eee6df0ef90c11.pdf" target="_new">[Slide] </a>
<a href="http://docs.wixstatic.com/ugd/6db7b6_df9ea13365214caa8dce3e7aae260a45.pdf" target="_new">[Poster] </a>
</div></td>
</tr>

<tr>
<td width="10%">
<img src="./JaeShin_homepage/vpgNet.png" width="200" height="100">
</td>
<td width="90%" bgcolor="#e9e9e9"><div id="pub_gray"> 
<h5>VPGNet: Vanishing Point Guided Network for Lane and Road Marking Detection and Recognition</h5>
Seokju Lee, Junsik Km, <b>Jae Shin Yoon</b>, Seunghak Shin, Oleksandr Bailo, Namil Kim, Tae-hee Lee, Hyun Seok Hong, Seung-Hoon Han, In So Kweon<br>
IEEE International Conference on Computer Vision (<b>ICCV</b>) 2017 <br>
<a href="https://arxiv.org/abs/1710.06288" target="_new">[PDF] </a>
<a href="https://github.com/SeokjuLee/VPGNet" target="_new">[Project Page] </a>
</div></td>
</tr>

<tr>
<td width="10%">
<img src="./JaeShin_homepage/wacv2016.png" width="200" height="100">
</td>
<td width="90%" bgcolor="#ffffff"><div id="pub"> 
<h5>Robust Road Marking Detection and Recognition Using Density-Based Grouping and Machine Learning Techniques</h5>
Oleksandr Bailo, Seokju Lee, Francois Rameau, <b>Jae Shin Yoon</b>, In So Kweon<br>
IEEE Winter Conference on Application and Computer Vision (<b>WACV</b>) 2017 <br>
<a href="http://docs.wixstatic.com/ugd/6db7b6_ea599a8489c64057966b0a3562bc49cb.pdf" target="_new">[PDF] </a>
</div></td>
</tr>



<!-- 2016 -->

<tr><td>
<!--
<h6>2016</h6>
-->
</td></tr>


<tr>
<td width="10%">
<img src="./JaeShin_homepage/thermal.png" width="200" height="100">
</td>
<td width="90%" bgcolor="#e9e9e9"><div id="pub_gray"> 
<h5>Thermal-Infrared based Drivable Region Detection</h5>
<b>Jae Shin Yoon</b>, Kibaek Park, Soonmin Hwang, Namil Kim, Yukyung Choi, Francois Rameau and
In So Kweon<br>
IEEE Intelligent Vehicle Symposium (<b>IV</b>) 2016 <br>
<a href="http://docs.wixstatic.com/ugd/6db7b6_50ee4b176657410ab6d86dd3e5d2d60b.pdf" target="_new">[PDF] </a>
<a href="https://sites.google.com/site/drivableregion/home" target="_new">[Project Page] </a>
<a href="https://drive.google.com/drive/u/1/folders/0BwKTPCXsfcLNMDhyMjBIcWRSMkE" target="_new">[Dataset] </a>
</div></td>
</tr>


<!-- 2015 -->

<tr><td>
<!--
<h6>2015</h6>
-->
</td></tr>



<tr>
<td width="10%">
<img src="./JaeShin_homepage/visual_recognition.png" width="200" height="100">
</td>
<td width="90%" bgcolor="#ffffff"><div id="pub"> 
<h5>All-Day Visual Place Recognition: Benchmark Dataset and Baseline</h5>
Yukyung Choi, Namil Kim, Kibak Park, Soonmin Hwang, <b>Jae Shin Yoon</b>,In So Kweon<br>
IEEE Computer Vision and Pattern Recognition Workshop 2015 on Visual Place Recognition in Changing Environments (<b>CVPR-VPRICE</b>) 2015<br>
<a href="http://docs.wixstatic.com/ugd/6db7b6_804b6941589d43449da1adbf1f8a2f7b.pdf" target="_new">[PDF] </a>
<a href="https://sites.google.com/site/ykchoicv/multispectral_vprice" target="_new">[Project Page] </a>
</div></td>
</tr>



<tr>
<td width="10%">
<img src="./JaeShin_homepage/lowcost.png" width="200" height="100">
</td>
<td width="90%" bgcolor="#e9e9e9"><div id="pub_gray"> 
<h5>Low-Cost Synchronization for Multispectral Cameras</h5>
Yukyung Choi, Namil Kim, Kibak Park, Soonmin Hwang, <b>Jae Shin Yoon</b>, In So Kweon<br>
IEEE International Conference on Ubiquitous Robots and Ambient Intelligence (<b>URAI</b>) 2015<br>
<a href="./JaeShin_homepage/lowcost.pdf" target="_new">[PDF] </a>
</div></td>
</tr>



<tr>
<td width="10%">
<img src="./JaeShin_homepage/geo_cali.png" width="200" height="100"> 
</td>
<td width="90%" bgcolor="#ffffff"><div id="pub"> 
<h5>Geometrical calibration of multispectral calibration</h5>
Namil Kim, Yukyung Choi, Soonmin Hwang, Kibaek Park, <b>Jae Shin Yoon</b>, In So Kweon<br>
IEEE International Conference on Ubiquitous Robots and Ambient Intelligence<br>
<a href="./JaeShin_homepage/geocali.pdf">[PDF] </a>
</div></td>
</tr>




<tbody><tr><td>
<h6>Technical Report</h6>
</td><tr>

<tr>
<td width="10%">
<img src="./JaeShin_homepage/cloth.gif"  width="200" height="105">
</td>
<td width = "90%" bgcolor="#e9e9e9"> <div id="pub_gray">
<h5>Neural 3D Clothes Retargeting from a Single Image</h5>
<b>Jae Shin Yoon</b>, <a href="http://www.kihwan23.com/">Kihwan Kim</a>, <a href="http://jankautz.com/">Jan Kautz</a>, and <a href="https://www-users.cs.umn.edu/~hspark/">Hyun Soo Park</a><br>
Technical Report 2021 <br>
<a href="https://arxiv.org/pdf/2102.00062.pdf" target="_new">[PDF]</a> 


<!--

</pre>
</div></td>
</tr>

<tr>
<td width="10%">
<img src="./JaeShin_homepage/cloth.gif"  width="200" height="105">
</td>
<td width="90%" bgcolor="#ffffff"><div id="pub"> 
<h5>Neural 3D Clothes Retargeting from a Single Image</h5>
<b>Jae Shin Yoon</b>, <a href="http://www.kihwan23.com/">Kihwan Kim</a>, <a href="http://jankautz.com/">Jan Kautz</a>, and <a href="https://www-users.cs.umn.edu/~hspark/">Hyun Soo Park</a><br>
Technical Report 2021 <br>
<a href="https://arxiv.org/pdf/2102.00062.pdf" target="_new">[PDF]</a> 
<a href="javascript:toggleHide(&#39;neural_cloth&#39;)">[BibTeX] </a>
<pre id="neural_cloth" class="bibtex" style="display:none">@article{yoon2021neural,
  title={Neural 3D Clothes Retargeting from a Single Image},
  author={Yoon, Jae Shin and Kim, Kihwan and Kautz, Jan and Park, Hyun Soo},
  journal={arXiv preprint arXiv:2102.00062},
  year={2021}
}

</pre>
</div></td>
</tr>


-->



</tbody></table>



<h2>
<font size = "4">Dataset
</h2>

<li> <a href="https://www.humbi-data.net/" target="_new"> Human Multiview Behavioral Imaging Dataset</a></li>
<ol>&nbsp;&nbsp;- Multiview images captured from 107 synchronized cameras</ol>
<ol>&nbsp;&nbsp;- 3D gaze, face, hand, body, clothing of 772 subjects</ol>
<li><a href="https://gorokee.github.io/jsyoon/dynamic_synth/" target="_new"> Dynamic Scene Multiview Dataset for Depth Estimation and View Synthesis </a></li>
<ol>&nbsp;&nbsp;- 13 Dynamic scenes captured from synchronized 12 multi-views and cell-phone camera</ol>
<ol>&nbsp;&nbsp;- Ground-truth for dynamic scene view synthesis and depth estimation</ol>
<li> <a href="https://github.com/SeokjuLee/VPGNet" target="_new"> Multi-Tasking Recognition for Autonomous Driving Scene</a></li>
<ol>&nbsp;&nbsp;- Ground-truth for classification, detection and segmentation of lanes, road markings, traffic signs, and vanishing point</ol>
<ol>&nbsp;&nbsp;- A variety of driving scene images captured from day and night in sunny, cloudy, and rainy days</ol>
<li> <a href="https://sites.google.com/view/multispectral/home" target="_new"> Multi-Sensor Imaging Dataset for Autonomous Driving Day and Night</a></li>
<ol>&nbsp;&nbsp;- Driving scene dataset of the calibrated stereo-vision, thermal camera, velodyne, gps, imu</ol>
<ol>&nbsp;&nbsp;- Large-scale day and night multi-sensor dataset</ol>
<li> <a href="https://sites.google.com/site/drivableregion/home" target="_new"> Thermal-RGB Road Segmentation Dataset</a></li>
<ol>&nbsp;&nbsp;- Synchronized and aligned thermal-rgb imaging dataset for road segmentation</ol>
<ol>&nbsp;&nbsp;- 5941 thermal-rgb image pairs with road mask annotation</ol>


<h2>
Research Internship
</h2>
<li><a href="https://research.adobe.com/" target="_blank">Adobe Research, San Jose (Virtual) </a>, USA | 2021 (June - Aug.)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;with <a href="http://www.duygu-ceylan.com/">Dr. Duygu Ceylan</a>, <a href="https://tuanfeng.github.io/">Dr. Tuanfeng Y. Wang</a>, <a href="https://research.adobe.com/person/jingwan-lu/">Dr. Jingwan (Cynthia) Lu</a>, <a href="https://eng.ucmerced.edu/people/jyang44/">Dr. Jimei Yang</a>, <a href="https://zhixinshu.github.io/">Dr. Zhixin Shu</a></li>
<li><a href="http://gvv.mpi-inf.mpg.de/GVV_contact.html" target="_blank">Graphics Vision & Video Lab, Max Planck Institute for Intelligent System, Saarland</a>, Germany | 2020.Aug. - 2021.Jan.<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;with <a href="http://gvv.mpi-inf.mpg.de/GVV_Team.html">Prof. Christian Theobalt</a>, <a href="https://lingjie0206.github.io/">Dr. Lingjie Liu</a>, <a href="https://people.mpi-inf.mpg.de/~golyanik/">Dr. Vladislav Golyanik</a>, <a href="https://people.mpi-inf.mpg.de/~ksarkar/">Dr. Kripasindhu Sarkar</a></li>
<li><a href="https://www.nvidia.com/en-us/research/" target="_blank">NVIDIA Research, Santa Clara</a>, USA | 2019 (Feb. - Aug.)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;with <a href="http://www.kihwan23.com/">Dr. Kihwan Kim</a>, <a href="http://alumni.soe.ucsc.edu/~orazio/">Dr. Orazio Gallo</a>, <a href="http://jankautz.com/">Dr. Jan Kautz</a></li>
<li><a href="https://www.oculus.com/research/" target="_blank">Facebook Reality Labs, Oculus, Pittsburgh</a>, USA | 2018 (June - Aug.)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;with <a href="https://scholar.google.com/citations?user=YvS3QpkAAAAJ&hl=en">Dr. Takaaki Shiratori</a>, <a href="https://sites.google.com/view/shoou-i-yu/home">Dr. Shoou-I Yu</a></li>
<li><a href="http://eng.kist.re.kr/kist_eng/main/" target="_blank">Korea Institute of Science and Technology <b>(KIST)</b></a>, Korea | 2017 (Mar. - Aug.)<br> 
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;with <a href="https://www.hwasup.net/">Dr. Hwasup Lim</a>, <a href="https://scholar.google.com/citations?user=6JAEII8AAAAJ&hl=en">Dr. Sang Chul Ahn</a></li>

<!--  

<a name="Academic Activities"></a>
<h2>
Academic Activities
</h2>

<div id="container">
<li> Reviewer </li>
	 
    <ol>&nbsp;&nbsp;- IEEE Conference on Computer Vision Pattern Recognition (<b>CVPR</b>) | 2018, 2019, 2020, 2021, 2022 </ol>
<ol>&nbsp;&nbsp;- Conference on Neural Information Processing System (<b>NeurIPS</b>) | 2022 </ol>    
	<ol>&nbsp;&nbsp;- Conference on Neural Information Processing System (<b>NeurIPS</b>) Datasets and Benchmarks Track | 2021 </ol>
    <ol>&nbsp;&nbsp;- Special Interest Group on Computer Graphics and Interactive Techniques (<b>SIGGRAPH</b>) | 2021 </ol>
    <ol>&nbsp;&nbsp;- IEEE European Conference on Computer Vision (<b>ECCV</b>) | 2020, 2022 </ol>
    <ol>&nbsp;&nbsp;- IEEE International Conference on Computer Vision (<b>ICCV</b>) | 2019, 2021 </ol>
    <ol>&nbsp;&nbsp;- International Conference on Learning Representations (<b>ICLR</b>) | 2022 </ol> 
    <ol>&nbsp;&nbsp;- AAAI Conference on Artificial Intelligence (<b>AAAI</b>) | 2020, 2021, 2022 </ol>
    <ol>&nbsp;&nbsp;- Eurographics Computer Graphics Forum (<b>CGF</b>) | 2021 </ol>
    <ol>&nbsp;&nbsp;- IEEE Winter Conference on Applications of Computer Vision (<b>WACV</b>) | 2020, 2021, 2022 </ol>
    <ol>&nbsp;&nbsp;- International Conference on 3D Vision (<b>3DV</b>) | 2021</ol>
    <ol>&nbsp;&nbsp;- IEEE Asian Conference on Computer Vision (<b>ACCV</b>) | 2018, 2020 </ol>
    <ol>&nbsp;&nbsp;- IEEE Intelligent Vehicle Symposium (<b>IV</b>) | 2016</ol>
    <ol>&nbsp;&nbsp;- Computer and Graphics | 2021 </ol>
    <ol>&nbsp;&nbsp;- IEEE Transactions on Neural Networks and Learning Systems | 2022 </ol>
    <ol>&nbsp;&nbsp;- IEEE Geoscience and Remote Sensing Letters | 2020 </ol>
    <ol>&nbsp;&nbsp;- IEEE Transactions on Multimedia | 2018 </ol>
    <ol>&nbsp;&nbsp;- Autonomous Robots | 2020 </ol>
    <ol>&nbsp;&nbsp;- CVPR Workshop on Computer Vision for Animal Behavior Tracking and Modeling | 2021, 2022 </ol>
    <ol>&nbsp;&nbsp;- ICML Workshop on Self-Supervised Learning for Reasoning and Perception | 2021 </ol>
    <ol>&nbsp;&nbsp;- NeurIPS Workshop on Self-Supervised Learning: Theory and Practice | 2020, 2021</ol>
 
</div>

-->  


<a name="Invited Talk"></a>
<h2>
Invited Talk
</h2>

<div id="container">
<li> MotionMix: Animating Human from a Single Image</li>
  <ol>&nbsp;&nbsp;- Adobe Max Sneak [<a href="https://youtu.be/nK6olXpOh1g/" target="_blank">Link</a>] | 2022</ol>
<li> Metaverse in the Wild: Modeling Adapting and Rendering of 3D Human Avatars from a Single Camera</li>
  <ol>&nbsp;&nbsp;- Adobe Research (job talk), hosted by Dr. Cynthia Lu | 2022</ol>
  <ol>&nbsp;&nbsp;- DGIST, AI Colloquium, hosted by Prof. Sunghoon Im | 2022</ol>
  <ol>&nbsp;&nbsp;- UST, Global Mentoring Conference, hosted by Prof. SangChul Ahn | 2023</ol>	
<li> Novel View Synthesis from Dynamic Scenes</li>
  <ol>&nbsp;&nbsp;- CVPR Tutorial on Novel View Synthesis: From Depth-Based Warping to Multi-Plane Images and Beyond [<a href="https://nvlabs.github.io/nvs-tutorial-cvpr2020/" target="_blank">Link</a>, <a href="./JaeShin_homepage/slide.pdf" target="_blank">Slide</a>] | 2020 </ol>
  <ol>&nbsp;&nbsp;- CVPR Workshop on 3D Scene Understanding for Vision, Graphics, and Robotics (Oral Presentation) [<a href="https://scene-understanding.com/" target="_blank">Link</a>] | 2020 </ol>

<li> Self-Supervised Adaptation of High-Fidelity Face Models for Monocular Performance Tracking</li>
  <ol>&nbsp;&nbsp;- CVPR oral presentation [<a href="https://youtu.be/j7A83F6PRAE?t=1097" target="_blank">Video</a>] | 2019 </ol>
  <ol>&nbsp;&nbsp;- Think Tank Team at Samsung Research America in Mountain View, hosted by Dr. Abhijit Bendale  | 2019 </ol>
  <ol>&nbsp;&nbsp;- UofM 50th Anniversary Research Showcase (Poster) </a> | 2019 </ol>
  <li> 3D Human Behavioral Imaging</li>
  <ol>&nbsp;&nbsp;- University of Minnesota (UMN) Graduate Research and Discussion Seminars (GRaDS) | 2019 </ol>
    <ol>&nbsp;&nbsp;- Seoul National Univeristy (SNU) Vision Seminar, hosted by Prof. Bohyung Han | 2019 </ol>
    <ol>&nbsp;&nbsp;- Pohang University of Science and Technology (POSTECH) Computer Vision group, hosted by Prof. Minsu Cho | 2019</ol>
    <ol>&nbsp;&nbsp;- Korea Institute of Science and Technology (KIST), hosted by Dr. Hwasup Lim | 2019</ol>
<li> 3D Semantic Trajectory Reconstruction from 3D Pixel Continuum</li>
    <ol>&nbsp;&nbsp;- Amazon Graduate Research Symposium (poster) | 2019 </ol>
    <ol>&nbsp;&nbsp;- University of Minnesota (UMN) <a href="https://sites.google.com/view/vcai/home?authuser=0" target="_blank">VCAI Seminar</a> | 2018 </ol>
<li> Semantics on the Road for All-day Autonomous Driving </li>
    <ol>&nbsp;&nbsp;- Hyundai Top Talent Forum | 2018 </ol>


</div>




<a name="Honors & Awards"></a>
<h2>
Honors & Awards
</h2>

<div id="container">
<li> Doctoral Consortium at CVPR | 2022 </li>
<li> Outstanding Reviewer Award, IEEE Conference on Computer Vision and Pattern Recognition | 2021</li>
<li> Doctoral Dissertation Fellowship (DDF), University of Minnesota ($ 25,000) | 2021</li>
<li> Top Graduate Research Award, UofM Graduate Research and Discussion Seminar (GRaDS) | 2019 </li>
<li> Summa Cum Laude, Hanyang University (GPA: 4.08/4.5) (94.6 %), Dept. of EE, Hanyang University | 2015</li>
<li> Hanyang Brain scholarship, Dept. of EE, Hanyang University ($1,440) | 2014</li>
<li> Prize for the top first percentile GPA, Hanyang University ($3,740) 
<li> Miraeasset Park Hyeon-Joo foundation scholarship for exchange student ($8,565) <a href="./JaeShin_homepage/mirae.pdf" target="_blank">[Certificate]</a> | 2012 </li>
<li> Qualcomm Scholarship for outstanding engineer, Qualcomm corp ($3,740) <a href="./JaeShin_homepage/qualcom_rotated.pdf" target="_blank">[Certificate]</a> | 2011 </li>
</div>

<a name ="Patent"></a>
<h2>
Patent
</h2>
<div id="container">
<li> View Synthesis for Dynamic Scenes (US Patent 11,546,568, 2023)</li>
</div>
<div id="container">
<li> Vanishing-Guided Lane and Road Marking Detection via Multi-Task Network</li>
  <ol>&nbsp;&nbsp;&nbsp;<font size ="2"> Application date: Oct. 31, 2016 (US Provisional Application 52/414,951)</ol> 
</div>


</tbody></table>





</center></body></html>
