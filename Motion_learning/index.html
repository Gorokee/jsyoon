
<!-- saved from url=(0045)https://www-users.cse.umn.edu/~jsyoon/3dface/ -->
<html xmlns="http://www.w3.org/1999/xhtml" class="gr__cs_cmu_edu"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

<title>Motion Dependent Appearance</title>
</head>

<body data-gr-c-s-loaded="true" data-new-gr-c-s-check-loaded="14.1052.0" data-gr-ext-installed="">
<table width="916" ,="" align="center">
  <tbody><tr>
    <td width="908"><h1 align="center"><font size="6" ,="" font="font" face="Trajans Pro"><strong>Learning Motion-Dependent Appearance for High-Fidelity Rendering of Dynamic Humans from a Single Camera</strong></font></h1>

<p align="center"><font size="3" ,="" font="font" face="Georgia"><a href="https://www-users.cs.umn.edu/~jsyoon/">Jae Shin Yoon</a><sup>1,2</sup>, <a href="http://www.duygu-ceylan.com/">Duygu Ceylan</a><sup>2</sup>, <a href="https://tuanfeng.github.io/">Tuanfeng Y. Wang</a><sup>2</sup>, <a href="https://research.adobe.com/person/jingwan-lu/">Jingwan Lu</a><sup>2</sup>, <a href="https://research.adobe.com/person/jimei-yang/">Jimei Yang</a><sup>2</sup>, <a href="https://zhixinshu.github.io/">Zhixin Shu</a><sup>2</sup> and <a href="https://www-users.cs.umn.edu/~hspark/">Hyun Soo Park</a><sup>1</sup></font></p>
      <p align="center"><font size="3" ,="" font="font" face="Georgia"><sup>1</sup>University of Minnesota &nbsp; &nbsp; &nbsp; &nbsp;
        <sup>2</sup>Adobe Research</font></p>
      <p align="center"><img src="./index_files/teaser.svg" alt="" width="800"></p>
<!--      <p align="center"><font size="3" ,="" font="font" face="Georgia">Figure 1: Given surface normal and velocity of a 3D body model, our method synthesizes subject-specific surface normal and appearance. We specifically focus on synthesis of plausible dynamic appearance by learning an effective 3D motion descriptor. </font></p> -->
      <p align="left"><font size="3" ,="" font="font" face="Georgia"><b>Abstract </b></font></p>
      <p align="justify"><font size="3" ,="" font="font" face="Georgia">Appearance of dressed humans undergoes a complex geometric transformation induced not only by the static pose but also by its dynamics, i.e., there exists a number of cloth geometric configurations given a pose depending on the way it has moved. Such appearance modeling conditioned on motion has been largely neglected in existing human rendering methods, resulting in rendering of physically implausible motion. A key challenge of learning the dynamics of the appearance lies in the requirement of a prohibitively large amount of observations. In this paper, we present a compact motion representation by enforcing equivariance---a representation is expected to be transformed in the way that the pose is transformed. We model an equivariant encoder that can generate the generalizable representation from the spatial and temporal derivatives of the 3D body surface. This learned representation is decoded by a compositional multi-task decoder that renders high fidelity time-varying appearance. Our experiments show that our method can generate a temporally coherent video of dynamic humans for unseen body poses and novel views given a single view video.</font>
      <p align="left"><font size="3" ,="" font="font" face="Georgia"><b>System Overview </b></font></p>
      <p align="center"><img src="./index_files/system.svg" alt="" width="1000"></p>
<!--       <p align="center"><font size="3" ,="" font="font" face="Georgia">Figure 2: The overview of our human rendering pipeline. </font></p> 

	</p><p align="center"><img src="./index_files/perf.gif" alt="" height="160"> <img src="./index_files/perf3.gif" alt="" height="160">  <img src="./index_files/perf4.gif" alt="" height="160"></p>
	  <p align="center"><font size="3" ,="" font="font" face="Georgia">Figure 2: Facial performance tracking from a monocular video. </font></p>
<br>
      <p align="center"><img src="./index_files/headpose.gif" alt="" height="220"> <img src="./index_files/multiview.gif" alt="" height="220"></p>
	  <p align="center"><font size="3" ,="" font="font" face="Georgia">Figure 3: Facial performance tracking with headpose and multiview rendering. </font></p>-->

 
<p align="left"><font size="3" ,="" font="font" face="Georgia" ><b>Results </b></font></p>	  
 	  <table width="1000" border="0">

   <tbody><tr>

    <td width="480" ,="" align="center"><font size="2" font="font" face="Arial, Helvetica">
	<video loop="" autoplay="" muted="" width="480">
					<source src="./index_files/teaser11.mp4" type="video/mp4">
	</video><div>

     
  </font></p></div></font></td>

    <td width="480" ,="" align="center"><font size="2" font="font" face="Arial, Helvetica">
	<video loop="" autoplay="" muted="" width="480">
					<source src="./index_files/teaser22.mp4" type="video/mp4">
	</video><div>
     
  </font></p></div></font></td>

	</tr>
 </tbody></table>



      
<p align="left"><font size="3" ,="" font="font" face="Georgia"><b>Application </b></font></p>


 	  <table width="1000" border="0">
   <tbody><tr>
    <td width="333" ,="" align="center"><font size="2" font="font" face="Arial, Helvetica">
	<video loop="" autoplay="" muted="" width="333">
					<source src="./index_files/application1.mp4" type="video/mp4">
	</video><div>
<p align="left"><font size="2" ,="" font="font" face="Georgia">&nbsp; &nbsp; &nbsp; &nbsp;&nbsp; &nbsp; &nbsp;  &nbsp;&nbsp; &nbsp; &nbsp; &nbsp; Bullet time effect & novel view synthesis

  </font></p></div></font></td>

    <td width="333" ,="" align="center"><font size="2" font="font" face="Arial, Helvetica">
	<video loop="" autoplay="" muted="" width="333">
					<source src="./index_files/application2.mp4" type="video/mp4">
	</video><div>

<p align="left"><font size="2" ,="" font="font" face="Georgia">&nbsp; &nbsp; &nbsp; &nbsp;&nbsp; &nbsp; &nbsp;  &nbsp;&nbsp; &nbsp; &nbsp; &nbsp;&nbsp; &nbsp; &nbsp; &nbsp;&nbsp; &nbsp; &nbsp;  &nbsp;&nbsp; &nbsp; &nbsp; &nbsp;Motion Transfer 
     
  </font></p></div></font></td>

    <td width="333" ,="" align="center"><font size="2" font="font" face="Arial, Helvetica">
	<video loop="" autoplay="" muted="" width="333">
					<source src="./index_files/application3.mp4" type="video/mp4">
	</video><div>
<p align="left"><font size="2" ,="" font="font" face="Georgia">&nbsp; &nbsp; &nbsp; &nbsp;&nbsp; &nbsp; &nbsp;  &nbsp;&nbsp; &nbsp; &nbsp; &nbsp;&nbsp; &nbsp; &nbsp; &nbsp;&nbsp; &nbsp; &nbsp;  &nbsp;&nbsp; &nbsp; &nbsp; &nbsp;Relighting 
     
  </font></p></div></font></td>

	</tr>
 </tbody></table>



<br>
      <p align="left"><font size="3" ,="" font="font" face="Georgia"><b>Paper </b></font></p>		
      <p>Jae Shin Yoon, Duygu Ceylan, Tuanfeng Y. Wang, Jingwan Lu, Jimei Yang, Zhixin Shu, and Hyun Soo Park "Learning Motion-Dependent Appearance for High-Fidelity Rendering of Dynamic Humans from a Single Camera", CVPR 2022 [<a href="https://arxiv.org/pdf/2203.12780.pdf">PDF</a>, <a href="https://www.dropbox.com/s/6zm0szi4br7yfys/cvpr_2022_bib.txt?dl=0">Bibtex</a>]</p>

      <p align="left"><font size="3" ,="" font="font" face="Georgia"><b>Video </b></font></p>		  
	<iframe width="973" height="547" src="https://www.youtube.com/embed/NWg1mRl22Ow" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
	
<!--
<iframe width="640" height="360" src="./index_files/4KgO54JvOE0.html" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe>
	  <p><b>Video (oral presentation with voice)</b></p>
<iframe width="640" height="360" src="./index_files/j7A83F6PRAE.html" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe>
	  <p><b>Supplementary video</b></p>
<iframe width="640" height="360" src="./index_files/kZNdiIX5WB4.html" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe>
649 365
-->








  </td></tr>
</tbody></table>


<span class="gr__tooltip"><span class="gr__tooltip-content"></span><i class="gr__tooltip-logo"></i><span class="gr__triangle"></span></span>

<!--
	  <p><b>Video (supplementary)</b></p>

	 <iframe width="640" height="360" src="./sample_files/onPjIXGUwIU.html" frameborder="0" allowfullscreen=""></iframe>
      <a href="">video download</a>
 <iframe width="640" height="360" src="./sample_files/onPjIXGUwIU.html" frameborder="0" allowfullscreen=""></iframe>

   <iframe width="640" height="360" src="https://www.youtube.com/embed/j7A83F6PRAE" frameborder="0" gesture="media" allow="encrypted-media" allowfullscreen></iframe>

-->

</body><grammarly-desktop-integration data-grammarly-shadow-root="true"></grammarly-desktop-integration></html>