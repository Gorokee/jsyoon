<html xmlns="http://www.w3.org/1999/xhtml" class="gr__cs_cmu_edu"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

<title>Facial Performance Tracking</title>
</head>

<body data-gr-c-s-loaded="true">
<table width="916" ,="" align="center">
  <tbody><tr>
    <td width="908"><h1 align="center"><font size="6" ,="" font="font" face="Trajans Pro"><strong>Self-Supervised Adaptation of High-Fidelity Face Models for Monocular Performance Tracking </strong></font></h1>

<p align="center"><font size="3" ,="" font="font" face="Georgia"><a href="https://www-users.cs.umn.edu/~jsyoon/">Jae Shin Yoon</a><sup>1</sup>, <a href="https://www.cs.cmu.edu/~siratori/">Takaaki Shiratori</a><sup>2</sup>, <a href="https://sites.google.com/view/shoou-i-yu/home">Shoou-I Yu</a><sup>2</sup>, and <a href="https://www-users.cs.umn.edu/~hspark/">Hyun Soo Park</a><sup>1</sup></font></p>
      <p align="center"><font size="3" ,="" font="font" face="Georgia"><sup>1</sup>University of Minnesota &nbsp; &nbsp; &nbsp; &nbsp;
        <sup>2</sup>Facebook Reality Labs, Pittsburgh</font></p>
      <p align="center"><img src="./files/teaser.png" alt="" width="800"></p>
      <p align="center"><font size="3" ,="" font="font" face="Georgia">Figure 1: We present a method to render a high-fidelity 3D face model from an unconstrained monocular video. </font></p>
      <p align="left"><font size="3" ,="" font="font" face="Georgia"><b>Abstract </b></font></p>
      <p align="justify"><font size="3" ,="" font="font" face="Georgia">Improvements in data-capture and face modeling techniques have enabled us to create high-fidelity realistic face models. However, driving these realistic face models requires special input data, e.g., 3D meshes and unwrapped textures. Also, these face models expect clean input data taken under controlled lab environments, which is very different from data collected in the wild. All these constraints make it challenging to use the high-fidelity models in tracking for commodity cameras. In this paper, we propose a self-supervised domain adaptation approach to enable the animation of high-fidelity face models from a commodity camera. Our approach first circumvents the requirement for special input data by training a new network that can directly drive a face model just from a single 2D image. Then, we overcome the domain mismatch between lab and uncontrolled environments by performing self-supervised domain adaptation based on "consecutive frame texture consistency" based on the assumption that the appearance of the face is consistent over consecutive frames, avoiding the necessity of modeling the new environment such as lighting or background. Experiments show that we are able to drive a high-fidelity face model to perform complex facial motion from a cellphone camera without requiring any labeled data from the new domain.</font>
      <p align="center"><img src="./files/perf.gif" alt="" height="160"> <img src="./files/perf3.gif" alt="" height="160">  <img src="./files/perf4.gif" alt="" height="160"></p>
	  <p align="center"><font size="3" ,="" font="font" face="Georgia">Figure 2: Facial performance tracking from a monocular video. </font></p>
<br>
      <p align="center"><img src="./files/headpose.gif" alt="" height="220"> <img src="./files/multiview.gif" alt="" height="220"></p>
	  <p align="center"><font size="3" ,="" font="font" face="Georgia">Figure 3: Facial performance tracking with headpose and multiview rendering. </font></p>


<br>
		
      <p>Jae Shin Yoon, Takaaki Shiratori, Shoou-I Yu, and Hyun Soo Park "Self-Supervised Adaptation of High-Fidelity Face Models for Monocular Performance Tracking", CVPR 2019 (Oral presentation)</b></font> [<a href="http://openaccess.thecvf.com/content_CVPR_2019/html/Yoon_Self-Supervised_Adaptation_of_High-Fidelity_Face_Models_for_Monocular_Performance_Tracking_CVPR_2019_paper.html">PDF</a><a href=""></a>, <a href="http://openaccess.thecvf.com/content_CVPR_2019/html/Yoon_Self-Supervised_Adaptation_of_High-Fidelity_Face_Models_for_Monocular_Performance_Tracking_CVPR_2019_paper.html">PDF_supplementary</a>, <a href="./files/poster_3dface.pdf">Poster</a>, <a href="http://openaccess.thecvf.com/content_CVPR_2019/html/Yoon_Self-Supervised_Adaptation_of_High-Fidelity_Face_Models_for_Monocular_Performance_Tracking_CVPR_2019_paper.html">Bibtex</a>]</p>
	  
	  <p><b>Video (oral presentation)</b></p>
<iframe width="640" height="360" src="https://www.youtube.com/embed/4KgO54JvOE0" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
	  <p><b>Video (oral presentation with voice)</b></p>
<iframe width="640" height="360" src="https://www.youtube.com/embed/j7A83F6PRAE?start=1098" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
	  <p><b>Supplementary video</b></p>
<iframe width="640" height="360" src="https://www.youtube.com/embed/kZNdiIX5WB4" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>





  </tr>
</tbody></table>


</body><span class="gr__tooltip"><span class="gr__tooltip-content"></span><i class="gr__tooltip-logo"></i><span class="gr__triangle"></span></span></html>

<!--
	  <p><b>Video (supplementary)</b></p>

	 <iframe width="640" height="360" src="./sample_files/onPjIXGUwIU.html" frameborder="0" allowfullscreen=""></iframe>
      <a href="">video download</a>
 <iframe width="640" height="360" src="./sample_files/onPjIXGUwIU.html" frameborder="0" allowfullscreen=""></iframe>

   <iframe width="640" height="360" src="https://www.youtube.com/embed/j7A83F6PRAE" frameborder="0" gesture="media" allow="encrypted-media" allowfullscreen></iframe>

-->

