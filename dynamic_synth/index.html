
<!-- saved from url=(0044)https://www-users.cs.umn.edu/~jsyoon/3dface/ -->
<html xmlns="http://www.w3.org/1999/xhtml" class="gr__cs_cmu_edu"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

<title>Dynamic Scene View Synthesis</title>
</head>

<link href="./Jae Shin Yoon&#39;s Homepage - University of Minnesota._files/css" rel="stylesheet" type="text/css">
<link href="./Jae Shin Yoon&#39;s Homepage - University of Minnesota._files/css(1)" rel="stylesheet" type="text/css">


<body data-gr-c-s-loaded="true">
<table width="916" ,="" align="center">
  <tbody><tr>
    <td width="908"><h1 align="center"><font size="6" ,="" font="font" face="Trajans Pro"><strong>Novel View Synthesis of Dynamic Scenes with Globally Coherent Depths from a Monocular Camera</strong></font></h1>

<p align="center"><font size="3" ,="" font="font" face="Georgia"><a href="https://www-users.cs.umn.edu/~jsyoon/">Jae Shin Yoon</a><sup>1</sup>, <a href="http://www.kihwan23.com/">Kihwan Kim</a><sup>2</sup>, <a href="http://alumni.soe.ucsc.edu/~orazio/">Orazio Gallo</a><sup>2</sup>, <a href="https://www-users.cs.umn.edu/~hspark/">Hyun Soo Park</a><sup>1</sup>, and <a href="http://jankautz.com/">Jan Kautz</a><sup>2</sup></font></p>
      <p align="center"><font size="3" ,="" font="font" face="Georgia"><sup>1</sup>University of Minnesota &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;
        <sup>2</sup>NVIDIA</font></p>
      <p align="center"><img src="./dynamic_synthesis/teaser.png" alt="" width="1010"></p>
      <p align="left"><font size="3" ,="" font="font" face="Georgia">Figure 1: We present a new method to synthesize an image from arbitrary views and times given a collection of images of a dynamic scene. <b>(left)</b>
	  A dynamic scene is captured from a monocular camera from the locations <b>V</b><sub>0</sub> to <b>V</b><sub>k</sub>. Each image captures people jumping at each time step (<b>t</b>=0 to <b>t</b>=k). 
	  <b>(Middle)</b> A novel view from an arbitrary location between <b>V</b><sub>0</sub> and <b>V</b><sub>1</sub> (denoted as an orange frame) is synthesized with the dynamic contents observed at the time <b>t</b>=k. The estimated depth at <b>V</b><sub>k</sub> is shown in the inset. <b>(Right)</b> For the novel view (orange frame), we can also synthesize the dynamic content that appeared across any views in different time (traces of the foreground in each time step are shown). </font></p>
      <p align="left"><font size="3" ,="" font="font" face="Georgia"><b>Abstract </b></font></p>
      <p align="justify"><font size="3" ,="" font="font" face="Georgia">
	  This paper presents a new method to synthesize an image from arbitrary views and times given a collection of images of a dynamic scene.
A key challenge for the novel view synthesis arises from dynamic scene reconstruction where epipolar geometry does not apply to the local motion of dynamic contents. 
To address this challenge, we propose to combine the depth from single view (DSV) and the depth from multi-view stereo (DMV), where DSV is complete,
 i.e., a depth is assigned to every pixel, yet view-variant in its scale, while DMV is view-invariant yet incomplete. Our insight is that although its scale and quality are inconsistent with other views, 
 the depth estimation from a single view can be used to reason about the globally coherent geometry of dynamic contents. We cast this problem as learning to correct the scale of DSV, 
 and to refine each depth with locally consistent motions between views to form a coherent depth estimation. We integrate these tasks into a depth fusion network in a self-supervised fashion. 
 Given the fused depth maps, we synthesize a photorealistic virtual view in a specific location and time with our deep blending network that completes the scene and renders the virtual view. 
 We evaluate our method of depth estimation and view synthesis on a diverse real-world dynamic scenes and show the outstanding performance over existing methods.  
	  </font>
<!--
	  <br><br>
	  <p align="center"><img src="./dynamic_synthesis/dfnet.png" alt="" width="1010"></p>
      <p align="left"><font size="3" ,="" font="font" face="Georgia">Figure 2: Our depth fusion network (DFNet) predicts a complete and view invariant depth map by fusing the depth from single view prediction (DSV) and the depth from multi-view stereo (DMV) with the image. 
	  DFNet is self-supervised by minimizing the background depth consistency with DMV (<i> L<sub>g</sub></i>), the relative depth consistency with DSV (<i> L<sub>l</sub></i>),
	  3D scene flow ((<i> L<sub>s</sub></i>), and spatial irregularity (<i> L<sub>s</sub></i>)</font></p>
-->
	 <br><br> 
      <p align="left"><font size="3" ,="" font="font" face="Georgia"><b>Paper</b></font></p>
	 <p>Jae Shin Yoon, Kihwan Kim, Orazio Gallo, Hyun Soo Park, and Jan Kautz "Novel View Synthesis of Dynamic Scenes with Globally Coherent Depths from a Monocular Camera", 
	  CVPR 2020 [<a href="../JaeShin_homepage/dynamic_view.pdf">Paper</a>, <a href="../JaeShin_homepage/dynamic_view_supple.pdf">PDF_supplementary</a>] </p>

	  
	  <br>
      <p align="left"><font size="3" ,="" font="font" face="Georgia"><b>Supplementary Video</b></font></p>
<iframe width="1024" height="576" src="https://www.youtube.com/embed/pTCkCGr2IH0" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>

      <p align="left"><font size="3" ,="" font="font" face="Georgia"><b>10 Minuate Overview</b> </font></p>
<iframe width="1024" height="576" src="https://www.youtube.com/embed/brza-yB7cVE" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>

	  
	  <!--[<a href="http://openaccess.thecvf.com/content_CVPR_2019/html/Yoon_Self-Supervised_Adaptation_of_High-Fidelity_Face_Models_for_Monocular_Performance_Tracking_CVPR_2019_paper.html">PDF</a><a href="https://www-users.cs.umn.edu/~jsyoon/3dface/"></a>, <a href="http://openaccess.thecvf.com/content_CVPR_2019/html/Yoon_Self-Supervised_Adaptation_of_High-Fidelity_Face_Models_for_Monocular_Performance_Tracking_CVPR_2019_paper.html">PDF_supplementary</a>, <a href="https://www-users.cs.umn.edu/~jsyoon/3dface/files/poster_3dface.pdf">Poster</a>, <a href="http://openaccess.thecvf.com/content_CVPR_2019/html/Yoon_Self-Supervised_Adaptation_of_High-Fidelity_Face_Models_for_Monocular_Performance_Tracking_CVPR_2019_paper.html">Bibtex</a>]</p>
-->
	  <br><br><br> 
      <p align="left"><font size="3" ,="" font="font" face="Georgia"><b>Dataset with ground-truth</b> <br>
	  : Input images of dynamic scenes, foreground masks, camera calibration, and GT of view synthesis (multiview images) and depth estimation </font></p>

	  <table width="1000" border="0">
   <tr>
    <td width="333", align="center"><font size = "2" font="font" face = "Arial, Helvetica">
	<video loop="" autoplay="" muted="" width="333" ">
					<source src="./dynamic_synthesis/jump.mp4" type="video/mp4">
	</video><div>
<!--	<p align="left"><img src="./dynamic_synthesis/jumping.gif" alt="" width="333"></p>--> 
<p align="left"><font size="2" ,="" font="font" face="Georgia">Jumping <a href="https://www.dropbox.com/sh/ws9khkjv7vnyub2/AADsQ5H8ixc_yEsiNarsjGhBa?dl=0">[dataset]</a> 
<!-- Jumping [dataset] [our resutls]-->
     
  </td>
  

    <td width="333", align="center"><font size = "2" font="font" face = "Arial, Helvetica">
	<video  loop="" autoplay="" muted="" width="333" >
					<source src="./dynamic_synthesis/skating.mp4" type="video/mp4">
	</video><div>
<p align="left"><font size="2" ,="" font="font" face="Georgia">Skating <a href="https://www.dropbox.com/sh/pp1phzamaxyl60j/AADU9y9LQSWmMpkP9yckE1ZKa?dl=0">[dataset]</a> 
<!-- Jumping [dataset] [our resutls]-->
     
  </td>

    <td width="333", align="center"><font size = "2" font="font" face = "Arial, Helvetica">
	<video loop="" autoplay="" muted="" width="333">
					<source src="./dynamic_synthesis/truck.mp4" type="video/mp4">
	</video><div>

<p align="left"><font size="2" ,="" font="font" face="Georgia">Truck <a href="https://www.dropbox.com/sh/svglgn553dei9dd/AAAy1lwNv29FCJ8eQzjR695Ma?dl=0">[dataset]</a> 
<!-- Jumping [dataset] [our resutls]-->
     
  </td>

	</tr>

 </table>

 
<br>


	  <table width="1000" border="0">
   <tr>
    <td width="333", align="center"><font size = "2" font="font" face = "Arial, Helvetica">
	<video  loop="" autoplay="" muted="" width="333" >
					<source src="./dynamic_synthesis/dynaface.mp4" type="video/mp4">
	</video><div>

<p align="left"><font size="2" ,="" font="font" face="Georgia">DynamicFace <a href="https://www.dropbox.com/sh/z90byp14nxfbsbw/AAAjFRxHEwVi98gbV6gHjfLLa?dl=0">[dataset]</a> 
<!-- Jumping [dataset] [our resutls]-->
     
  </td>

    <td width="333", align="center"><font size = "2" font="font" face = "Arial, Helvetica">
	<video loop="" autoplay="" muted="" width="333" >
					<source src="./dynamic_synthesis/umbrella.mp4" type="video/mp4">
	</video><div>

<p align="left"><font size="2" ,="" font="font" face="Georgia">Umbrella <a href="https://www.dropbox.com/sh/xbp9coi0lee80qy/AADG9F6fmdeQlxCZHkIUgfz0a?dl=0">[dataset]</a> 
<!-- Jumping [dataset] [our resutls]-->
     
  </td>

    <td width="333", align="center"><font size = "2" font="font" face = "Arial, Helvetica">
	<video loop="" autoplay="" muted="" width="333" >
					<source src="./dynamic_synthesis/balloon1.mp4" type="video/mp4">
	</video><div>

<p align="left"><font size="2" ,="" font="font" face="Georgia">Balloon1 <a href="https://www.dropbox.com/sh/qlhpjitoakghb1d/AADmUgq4nnEuVDbWOaxpjhnma?dl=0">[dataset]</a> 
<!-- Jumping [dataset] [our resutls]-->
     
  </td>

	</tr>
 </table>

 <br>
 	  <table width="1000" border="0">
   <tr>
    <td width="333", align="center"><font size = "2" font="font" face = "Arial, Helvetica">
	<video loop="" autoplay="" muted="" width="333" >
					<source src="./dynamic_synthesis/balloon2.mp4" type="video/mp4">
	</video><div>

<p align="left"><font size="2" ,="" font="font" face="Georgia">Balloon2 <a href="https://www.dropbox.com/sh/n5w7a1evbnmxzib/AAAH09_LJ8PHfLjqOdqSjsoAa?dl=0">[dataset]</a> 
<!-- Jumping [dataset] [our resutls]-->
     
  </td>

    <td width="333", align="center"><font size = "2" font="font" face = "Arial, Helvetica">
	<video loop="" autoplay="" muted="" width="333" >
					<source src="./dynamic_synthesis/teadybear.mp4" type="video/mp4">
	</video><div>

<p align="left"><font size="2" ,="" font="font" face="Georgia">Teadybear <a href="https://www.dropbox.com/sh/0182byj05p20h1p/AACPG1IxyCPVB9EOFsD_Sk39a?dl=0">[dataset]</a> 
<!-- Jumping [dataset] [our resutls]-->
     
  </td>

    <td width="333", align="center"><font size = "2" font="font" face = "Arial, Helvetica">
	<video loop="" autoplay="" muted="" width="333" >
					<source src="./dynamic_synthesis/playground.mp4" type="video/mp4">
	</video><div>

<p align="left"><font size="2" ,="" font="font" face="Georgia">Playground <a href="https://www.dropbox.com/sh/eg04jm1wcn5enez/AAC1vDqtqhTFOqVIaUGNiJC0a?dl=0">[dataset]</a> 
<!-- Jumping [dataset] [our resutls]-->
     
  </td>

	</tr>
 </table>

 
 <br><br>
  <p align="left"><font size="3" ,="" font="font" face="Georgia"><b>Dataset without ground-truth</b><br>
  : Input images of dynamic scenes, foreground masks, and camera calibration</font></p>

 	  <table width="1000" border="0">
   <tr>
    <td width="250", align="center"><font size = "2" font="font" face = "Arial, Helvetica">
	<video loop="" autoplay="" muted="" width="250" >
					<source src="./dynamic_synthesis/teatime.mp4" type="video/mp4">
	</video><div>

<p align="left"><font size="2" ,="" font="font" face="Georgia">Teatime <a href="https://www.dropbox.com/sh/bi0139ve1fq4pas/AAB2oJg9INcrgMAr543sj6ara?dl=0">[dataset]</a> 
<!-- Jumping [dataset] [our resutls]-->
     
  </td>

    <td width="250", align="center"><font size = "2" font="font" face = "Arial, Helvetica">
	<video loop="" autoplay="" muted="" width="250" >
					<source src="./dynamic_synthesis/jumping.mp4" type="video/mp4">
	</video><div>

<p align="left"><font size="2" ,="" font="font" face="Georgia">Feeling <a href="https://www.dropbox.com/sh/jhrz0nmavous6o3/AABZjt12dFBJ3nvcjWL_z4pDa?dl=0">[dataset]</a> 
<!-- Jumping [dataset] [our resutls]-->
     
  </td>

    <td width="250", align="center"><font size = "2" font="font" face = "Arial, Helvetica">
	<video loop="" autoplay="" muted="" width="250" >
					<source src="./dynamic_synthesis/hand.mp4" type="video/mp4">
	</video><div>

<p align="left"><font size="2" ,="" font="font" face="Georgia">Hand <a href="https://www.dropbox.com/sh/l1eyxn4farnk47z/AAD4wOtsCO3b5KqBJZNIDh0ya?dl=0">[dataset]</a> 
<!-- Jumping [dataset] [our resutls]-->
     
  </td>
    <td width="250", align="center"><font size = "2" font="font" face = "Arial, Helvetica">
	<video loop="" autoplay="" muted="" width="250" >
					<source src="./dynamic_synthesis/zebra.mp4" type="video/mp4">
	</video><div>

<p align="left"><font size="2" ,="" font="font" face="Georgia">Zebra <a href="https://www.dropbox.com/sh/f9pgnivh2ua5hxq/AAB9FteIBcD9cc3sKxo4PPgZa?dl=0">[dataset]</a> 
<!-- Jumping [dataset] [our resutls]-->
     
	</tr>
 </table>

 <br>
  <br>
   <br>
   

   *Full multiview or single-view videos can be found in <a href="https://www.dropbox.com/sh/4bqmops2lm6gg5i/AACwg_W9xfPxTXcSf64Ndx7pa?dl=0">here</a>. Note that you need to calibrate and undistort the images.<br>


   
<!--	    *please contact the first author if you need full videos for the above scenes: <a href="https://www-users.cs.umn.edu/~jsyoon/">Jae Shin Yoon</a>
-->
 
<!--	 <tr width="310", align="center"><font size = "2" font="font" face = "Arial, Helvetica">
		Jumping <div>
      <p align="left"><img src="./dynamic_synthesis/view_synth.gif" alt="" width="310"></p>
	  </tr>-->
	  
	  
	 <!-- 
      </p><p align="center"><img src="./dynamic_synthesis/perf.gif" alt="" height="160"> <img src="./Facial Performance Tracking_files/perf3.gif" alt="" height="160">  <img src="./Facial Performance Tracking_files/perf4.gif" alt="" height="160"></p>
	  <p align="center"><font size="3" ,="" font="font" face="Georgia">Figure 2: Facial performance tracking from a monocular video. </font></p>
<br>
      <p align="center"><img src="./dynamic_synthesis/headpose.gif" alt="" height="220"> <img src="./Facial Performance Tracking_files/multiview.gif" alt="" height="220"></p>
	  <p align="center"><font size="3" ,="" font="font" face="Georgia">Figure 3: Facial performance tracking with headpose and multiview rendering. </font></p>
-->
	  
	  
<br>
		
	  




<br><br>
  <p align="left"><font size="3" ,="" font="font" face="Georgia"><b>Reference</b> <br>
  : Please cite the following paper, if you use our dataset. <br>
   : Please use the dataset only for the research purpose. </font></p>
      

    <table width="1000" border="0">
   <tr>
   
<td width="80%" bgcolor="#e9e9e9" style="position: relative;top:0px; left:0px;"><div id="pub"> 
<p style="color:#000000;font-size: 15px; font-weight: regular; position: relative; top:4px; left:10px; width:1100px; line-height:130%">
@article{yoon2020dynamic,<br>
  title={Novel View Synthesis of Dynamic Scenes with Globally Coherent Depths from a Monocular Camera},<br>  
  author={Yoon, Jae Shin and Kim, Kihwan and Gallo, Orazio and Park, Hyun Soo and Kautz, Jan},<br>
  booktitle={The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},<br>
  month={June},<br>
  year={2020}<br>
}<br><br>
</p>

</div></td>
   </tr>
</table>

  
  
  

  </td></tr>
</tbody></table>


  



<span class="gr__tooltip"><span class="gr__tooltip-content"></span><i class="gr__tooltip-logo"></i><span class="gr__triangle"></span></span>

<!--
	  <p><b>Video (supplementary)</b></p>

	 <iframe width="640" height="360" src="./sample_files/onPjIXGUwIU.html" frameborder="0" allowfullscreen=""></iframe>
      <a href="">video download</a>
 <iframe width="640" height="360" src="./sample_files/onPjIXGUwIU.html" frameborder="0" allowfullscreen=""></iframe>

   <iframe width="640" height="360" src="https://www.youtube.com/embed/j7A83F6PRAE" frameborder="0" gesture="media" allow="encrypted-media" allowfullscreen></iframe>

-->

</body></html>